{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the input text: 1115394\n",
      "The first 100 characters are:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total number of characters in the input text: {len(text)}\")\n",
    "print(\"The first 100 characters are:\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a \"tokenizer\"\n",
    "char_to_ind = { v:i for i,v in enumerate(vocab) }\n",
    "ind_to_char = {i:v for i,v in enumerate(vocab) }\n",
    "\n",
    "#print(char_to_ind)\n",
    "#print(ind_to_char)\n",
    "\n",
    "def encode(text: str) -> list[int]:\n",
    "    return [char_to_ind[c] for c in text]\n",
    "\n",
    "def decode(tokens: list[int]) -> str:\n",
    "    return \"\".join([ind_to_char[i] for i in tokens])\n",
    "\n",
    "test_phrase=\"Hallo World\"\n",
    "assert decode(encode(test_phrase)) == test_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q3/z6h39zp11w50p3mn1mc7jfsc0000gn/T/ipykernel_20885/1695666474.py:2: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(tokens, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(tokens, dtype=torch.long)\n",
    "train_data = data[:int(0.9*len(data))]\n",
    "val_data = data[int(0.9*len(data)):]\n",
    "\n",
    "assert len(data) == len(train_data) + len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "context_size = 8\n",
    "\n",
    "def get_batch(split:str):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data)-batch_size,(batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8]) torch.Size([32, 8])\n",
      "u -> n\n",
      "un -> g\n",
      "ung ->  \n",
      "ung  -> w\n",
      "ung w -> a\n",
      "ung wa -> v\n",
      "ung wav -> e\n",
      "ung wave -> r\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "for i in range(x.shape[1]):\n",
    "    print(decode(x[0,0:i+1].tolist()) + \" -> \" + decode([y[0,i].item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigrammModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, tokens, targets=None):\n",
    "        logits = self.embedding(tokens)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, tokens, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(tokens)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs,num_samples=1)\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "        return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([32, 8])\n",
      "y: torch.Size([32, 8])\n",
      "y_hat: torch.Size([256, 65])\n",
      "loss: tensor(4.8180, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigrammModel(vocab_size)\n",
    "\n",
    "x,y = get_batch('train')\n",
    "\n",
    "print(\"x:\", x.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "y_hat, loss = m(x,y)\n",
    "print(\"y_hat:\", y_hat.shape)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XnWY,bNmZp\n"
     ]
    }
   ],
   "source": [
    "tokens = m.generate(torch.zeros(1,1,dtype=torch.long), 10)\n",
    "print(decode(tokens.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0 is 3.824690818786621\n",
      "loss at step 1000 is 3.085151195526123\n",
      "loss at step 2000 is 2.7088260650634766\n",
      "loss at step 3000 is 2.7192509174346924\n",
      "loss at step 4000 is 2.536932945251465\n",
      "loss at step 5000 is 2.5350654125213623\n",
      "loss at step 6000 is 2.5420680046081543\n",
      "loss at step 7000 is 2.6122944355010986\n",
      "loss at step 8000 is 2.5074551105499268\n",
      "loss at step 9000 is 2.424015760421753\n",
      "2.3745944499969482\n"
     ]
    }
   ],
   "source": [
    "for steps in range(10000):\n",
    "    x,y = get_batch(\"train\")\n",
    "    y_hat, loss = m(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"loss at step {steps} is {loss.item()}\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "e bu r t n caondo t sers allet.\n",
      "\n",
      "Fint,\n",
      "Frest; tofo youn wans mar KI ws, t ILamanitizils CHO:\n",
      "SThe Pers? mefea!\n",
      "HUSO lita h ly.\n",
      "\n",
      "Th mo gomaveld cand;\n",
      "Fllou cowhou tok d Fokima hon ASOPushay t Wh, Plonole s forengme ngha r alarn yos tue s\n",
      "Whatoe o:\n",
      "LII Inqunoryou\n",
      "in, sthermeeyorcant o,\n",
      "Ore at t ld, b\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(tokens=torch.zeros([1,1],dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigrammModel2(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_embed: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embed = n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, tokens, targets=None):\n",
    "        token_emb = self.embedding(tokens)\n",
    "        logits = self.lm_head(token_emb)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, tokens, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(tokens)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs,num_samples=1)\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "        return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0 is 4.3424906730651855\n",
      "loss at step 1000 is 2.510611057281494\n",
      "loss at step 2000 is 2.580935001373291\n",
      "loss at step 3000 is 2.460279941558838\n",
      "loss at step 4000 is 2.39436411857605\n",
      "loss at step 5000 is 2.4070651531219482\n",
      "loss at step 6000 is 2.4641172885894775\n",
      "loss at step 7000 is 2.5510239601135254\n",
      "loss at step 8000 is 2.4255409240722656\n",
      "loss at step 9000 is 2.580385208129883\n",
      "Final loss: 2.551116704940796\n",
      "\n",
      "NAnd de, pe itithecurerang,\n",
      "TES:\n",
      "\n",
      "TRCHAn enssir hiererardar I nd Cofomputrog thiousthavim te uenoustitha all, aye t es, Sadesopesshetechehe\n",
      "EY lldanveehiomamou, h ard ys,\n",
      "AUSeothith d?\n",
      "ULIABue brth'd hindg y\n",
      "Geaifl itent bllor hin urot m.\n",
      "S:\n",
      "CHef t y merd inecthow, t ' ke Plf lm at, se te l's, tspll\n"
     ]
    }
   ],
   "source": [
    "m = BigrammModel2(vocab_size, 32)\n",
    "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)\n",
    "\n",
    "for steps in range(10000):\n",
    "    x,y = get_batch(\"train\")\n",
    "    y_hat, loss = m(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"loss at step {steps} is {loss.item()}\")\n",
    "print(\"Final loss:\", loss.item())\n",
    "\n",
    "print(decode(m.generate(tokens=torch.zeros([1,1],dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
